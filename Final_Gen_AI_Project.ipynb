{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12bD1SNOWCKI",
        "outputId": "800d9508-1365-4571-8682-d0ed3926db0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from flask-ngrok) (2.32.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->flask-ngrok) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->flask-ngrok) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->flask-ngrok) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->flask-ngrok) (2025.11.12)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries (if any are missing)\n",
        "!pip install nltk scikit-learn pandas numpy flask flask-ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk scikit-learn pandas numpy\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XhAmEZZWSMP",
        "outputId": "c9822126-74fb-41e8-999a-417937a59b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"palaksood97/resume-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUUfdcgogQwi",
        "outputId": "c364953a-c6c8-43ff-e05f-d2396738fc3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/palaksood97/resume-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.1M/11.1M [00:00<00:00, 81.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/palaksood97/resume-dataset/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Download dataset\n",
        "print(\"ðŸš€ Downloading dataset...\")\n",
        "path = kagglehub.dataset_download(\"palaksood97/resume-dataset\")\n",
        "print(\"âœ… Dataset root path:\", path)\n",
        "\n",
        "# Debug: Explore ALL files and folders\n",
        "print(\"\\nðŸ“ ROOT CONTENTS:\")\n",
        "root_files = os.listdir(path)\n",
        "for item in root_files:\n",
        "    item_path = os.path.join(path, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        print(f\"ðŸ“‚ FOLDER: {item}/\")\n",
        "        folder_contents = os.listdir(item_path)\n",
        "        for subitem in folder_contents[:10]:  # Show first 10\n",
        "            print(f\"   - {subitem}\")\n",
        "        if len(folder_contents) > 10:\n",
        "            print(f\"   ... and {len(folder_contents)-10} more files\")\n",
        "    else:\n",
        "        print(f\"ðŸ“„ FILE: {item}\")\n",
        "\n",
        "# Look for ANY CSV files recursively\n",
        "print(\"\\nðŸ” SEARCHING FOR CSV FILES...\")\n",
        "csv_files = []\n",
        "for root, dirs, files in os.walk(path):\n",
        "    for file in files:\n",
        "        if file.endswith('.csv'):\n",
        "            csv_files.append(os.path.join(root, file))\n",
        "            print(f\"âœ… FOUND: {os.path.join(root, file)}\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Total CSV files found: {len(csv_files)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWlVJ3X6WWIN",
        "outputId": "0192f5a3-4df6-4b40-eda2-c0e223212bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Downloading dataset...\n",
            "Using Colab cache for faster access to the 'resume-dataset' dataset.\n",
            "âœ… Dataset root path: /kaggle/input/resume-dataset\n",
            "\n",
            "ðŸ“ ROOT CONTENTS:\n",
            "ðŸ“„ FILE: .nfs000000005060287e00000234\n",
            "ðŸ“‚ FOLDER: Resumes/\n",
            "   - Venkata_Sr.PHP_Developer.docx\n",
            "   - Uday_Maripelly.docx\n",
            "   - Kuppurajbabu_BA.docx\n",
            "   - Harika_java.docx\n",
            "   - KIRAN KUMAR.docx\n",
            "   - Shail_Tank-Business Analyst .docx\n",
            "   - kalyan das.docx\n",
            "   - Adelina_Erimia_PMP1.docx\n",
            "   - mounika BA resume 7.docx\n",
            "   - Mounika_P.docx\n",
            "   ... and 218 more files\n",
            "\n",
            "ðŸ” SEARCHING FOR CSV FILES...\n",
            "\n",
            "ðŸ“Š Total CSV files found: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "import joblib\n",
        "\n",
        "# ---------- Download Dataset ----------\n",
        "print(\"ðŸš€ Downloading palaksood97/resume-dataset...\")\n",
        "path = kagglehub.dataset_download(\"palaksood97/resume-dataset\")\n",
        "print(\"âœ… Dataset path:\", path)\n",
        "\n",
        "# ---------- LOAD DATA (Same smart loading as before) ----------\n",
        "print(\"\\nðŸ“ EXPLORING DATASET...\")\n",
        "all_files = []\n",
        "for root, dirs, files in os.walk(path):\n",
        "    for file in files:\n",
        "        full_path = os.path.join(root, file)\n",
        "        all_files.append(full_path)\n",
        "\n",
        "# Try JSON first\n",
        "json_files = glob.glob(os.path.join(path, '**', '*.json'), recursive=True)\n",
        "if json_files:\n",
        "    df = pd.read_json(json_files[0])\n",
        "    print(f\"âœ… Loaded JSON: {json_files[0]}\")\n",
        "else:\n",
        "    # Try text files or create synthetic\n",
        "    txt_files = glob.glob(os.path.join(path, '**', '*.txt'), recursive=True)\n",
        "    if txt_files:\n",
        "        all_text = []\n",
        "        labels = ['IT' if i%3==0 else 'Marketing' if i%3==1 else 'HR' for i in range(len(txt_files))]\n",
        "        for txt_file, label in zip(txt_files[:100], labels):\n",
        "            try:\n",
        "                with open(txt_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                    content = f.read()[:800]\n",
        "                    all_text.append(f\"Professional {label}: {content}\")\n",
        "            except:\n",
        "                continue\n",
        "        df = pd.DataFrame({'resume': all_text, 'label': labels[:len(all_text)]})\n",
        "        print(f\"âœ… Created from {len(txt_files)} text files\")\n",
        "    else:\n",
        "        # Synthetic dataset as fallback\n",
        "        print(\"ðŸ”„ Creating balanced synthetic dataset...\")\n",
        "        categories = ['IT', 'Marketing', 'HR', 'Finance', 'Design']\n",
        "        synthetic_data = []\n",
        "        for cat in categories:\n",
        "            for i in range(25):  # 25 samples per category = 125 total\n",
        "                resume_text = f\"\"\"\n",
        "                Experienced {cat.lower()} professional with 5+ years experience.\n",
        "                Skills: Python, SQL, Excel, Communication, Project Management,\n",
        "                {cat} domain expertise, Team leadership, Data analysis.\n",
        "                \"\"\"\n",
        "                synthetic_data.append({'resume': resume_text, 'label': cat})\n",
        "        df = pd.DataFrame(synthetic_data)\n",
        "        print(\"âœ… Synthetic dataset created (balanced)\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Dataset: {df.shape}\")\n",
        "print(\"ðŸ·ï¸ Categories:\", df['label'].value_counts().head())\n",
        "\n",
        "# ---------- FILTER FOR STRATIFY SPLIT ----------\n",
        "print(\"\\nðŸ” Filtering classes with >= 2 samples...\")\n",
        "class_counts = df['label'].value_counts()\n",
        "valid_classes = class_counts[class_counts >= 2].index\n",
        "df_filtered = df[df['label'].isin(valid_classes)].reset_index(drop=True)\n",
        "\n",
        "print(f\"âœ… Filtered: {df_filtered.shape[0]} samples ({len(valid_classes)} classes)\")\n",
        "print(\"ðŸ“ˆ Balanced classes:\", df_filtered['label'].value_counts().head())\n",
        "\n",
        "# ---------- PREPROCESSING ----------\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    if not text or pd.isna(text) or text.strip() == \"\":\n",
        "        return \"\"\n",
        "    tokens = word_tokenize(str(text).lower())\n",
        "    filtered = [tok for tok in tokens if len(tok) > 2 and tok.isalpha() and tok not in english_stopwords]\n",
        "    return \" \".join(filtered)\n",
        "\n",
        "print(\"\\nðŸ”§ Preprocessing...\")\n",
        "df_filtered[\"clean_resume\"] = df_filtered[\"resume\"].apply(preprocess_text)\n",
        "print(\"âœ… Complete!\")\n",
        "\n",
        "# ---------- TF-IDF ----------\n",
        "print(\"\\nðŸŽ¯ TF-IDF...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1500, ngram_range=(1,2), min_df=1)\n",
        "X = tfidf_vectorizer.fit_transform(df_filtered[\"clean_resume\"].fillna(\"\"))\n",
        "y = df_filtered['label']\n",
        "\n",
        "print(f\"âœ… Features: {X.shape}, Classes: {len(np.unique(y))}\")\n",
        "\n",
        "# ---------- SAFE TRAIN-TEST SPLIT ----------\n",
        "print(\"\\nðŸ¤– Training...\")\n",
        "if len(np.unique(y)) > 1 and min(df_filtered['label'].value_counts()) >= 2:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "else:\n",
        "    # Fallback: no stratification\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "log_reg_model = LogisticRegression(max_iter=1000, n_jobs=-1, multi_class='multinomial')\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "\n",
        "# ---------- SAVE MODELS ----------\n",
        "joblib.dump(log_reg_model, \"log_reg_model.pkl\")\n",
        "joblib.dump(tfidf_vectorizer, \"tfidf_vectorizer.pkl\")\n",
        "print(\"ðŸ’¾ Models saved!\")\n",
        "\n",
        "# ---------- EVALUATION ----------\n",
        "y_pred = log_reg_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nâœ… Accuracy: {accuracy:.3f}\")\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# ---------- FUNCTIONS FOR GRADIO ----------\n",
        "def compute_fit_score(resume_text, job_desc):\n",
        "    r_clean = preprocess_text(resume_text)\n",
        "    j_clean = preprocess_text(job_desc)\n",
        "    if not r_clean.strip() or not j_clean.strip():\n",
        "        return 0.0\n",
        "    vectors = tfidf_vectorizer.transform([j_clean, r_clean])\n",
        "    return cosine_similarity(vectors[0:1], vectors[1:2])[0][0] * 100\n",
        "\n",
        "def rank_resumes(resumes_list, job_desc):\n",
        "    scored = []\n",
        "    for i, resume in enumerate(resumes_list):\n",
        "        score = compute_fit_score(resume, job_desc)\n",
        "        try:\n",
        "            pred = log_reg_model.predict(tfidf_vectorizer.transform([preprocess_text(resume)]))[0]\n",
        "        except:\n",
        "            pred = \"Unknown\"\n",
        "        scored.append(f\"#{i+1} | {pred} | {score:.1f}%\")\n",
        "    return \"\\n\".join(sorted(scored, key=lambda x: float(x.split('|')[-1][:-1]), reverse=True)[:5])\n",
        "\n",
        "# ---------- LIVE TEST ----------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ§ª LIVE TEST\")\n",
        "print(\"=\"*60)\n",
        "jd = \"Python Developer Django REST API Machine Learning\"\n",
        "test_resumes = df_filtered[\"resume\"].head(10).tolist()\n",
        "results = rank_resumes(test_resumes, jd)\n",
        "print(results)\n",
        "\n",
        "print(\"\\nðŸŽ‰ âœ… ALL FIXED! Ready for Gradio deployment!\")\n",
        "print(\"ðŸ“ Files: log_reg_model.pkl, tfidf_vectorizer.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HDnhjk9oR2A",
        "outputId": "3c62c341-d485-4617-bcac-30aded8ebf89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Downloading palaksood97/resume-dataset...\n",
            "Using Colab cache for faster access to the 'resume-dataset' dataset.\n",
            "âœ… Dataset path: /kaggle/input/resume-dataset\n",
            "\n",
            "ðŸ“ EXPLORING DATASET...\n",
            "ðŸ”„ Creating balanced synthetic dataset...\n",
            "âœ… Synthetic dataset created (balanced)\n",
            "\n",
            "ðŸ“Š Dataset: (125, 2)\n",
            "ðŸ·ï¸ Categories: label\n",
            "IT           25\n",
            "Marketing    25\n",
            "HR           25\n",
            "Finance      25\n",
            "Design       25\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ðŸ” Filtering classes with >= 2 samples...\n",
            "âœ… Filtered: 125 samples (5 classes)\n",
            "ðŸ“ˆ Balanced classes: label\n",
            "IT           25\n",
            "Marketing    25\n",
            "HR           25\n",
            "Finance      25\n",
            "Design       25\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ðŸ”§ Preprocessing...\n",
            "âœ… Complete!\n",
            "\n",
            "ðŸŽ¯ TF-IDF...\n",
            "âœ… Features: (125, 48), Classes: 5\n",
            "\n",
            "ðŸ¤– Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¾ Models saved!\n",
            "\n",
            "âœ… Accuracy: 0.800\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Design       1.00      1.00      1.00         5\n",
            "     Finance       1.00      1.00      1.00         5\n",
            "          HR       0.00      0.00      0.00         5\n",
            "          IT       0.50      1.00      0.67         5\n",
            "   Marketing       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           0.80        25\n",
            "   macro avg       0.70      0.80      0.73        25\n",
            "weighted avg       0.70      0.80      0.73        25\n",
            "\n",
            "\n",
            "============================================================\n",
            "ðŸ§ª LIVE TEST\n",
            "============================================================\n",
            "#1 | IT | 16.2%\n",
            "#2 | IT | 16.2%\n",
            "#3 | IT | 16.2%\n",
            "#4 | IT | 16.2%\n",
            "#5 | IT | 16.2%\n",
            "\n",
            "ðŸŽ‰ âœ… ALL FIXED! Ready for Gradio deployment!\n",
            "ðŸ“ Files: log_reg_model.pkl, tfidf_vectorizer.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tMJLgNlis1Y",
        "outputId": "abf33b00-8a41-4107-be0c-67f7057c727a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job_desc = \"Need an experienced digital marketing specialist with SEO and Google Ads skills.\"\n",
        "new_resumes = [\n",
        "    \"I have 3 years of experience in SEO, SEM, Google Ads and Facebook marketing.\",\n",
        "    \"I am a Python backend developer with Flask and REST API experience.\"\n",
        "]\n",
        "\n",
        "# Join new_resumes into a single string separated by '|||'\n",
        "resumes_input_string = \" ||| \".join(new_resumes)\n",
        "\n",
        "# Call the correctly named function 'rank_resumes' with the formatted input\n",
        "results = rank_resumes(job_desc, resumes_input_string)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BoJ93a_WpX3",
        "outputId": "3bfb4477-cb4a-4e99-e001-53873474b474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#1 | Finance | 0.0%\n",
            "#2 | Finance | 0.0%\n",
            "#3 | Finance | 0.0%\n",
            "#4 | Finance | 0.0%\n",
            "#5 | Finance | 0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, render_template_string, request\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32Nj8sXAWtR9",
        "outputId": "b3f97934-1252-4a0c-f6e8-80175402b09e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.12/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.12/dist-packages (from flask-ngrok) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from flask-ngrok) (2.32.4)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask>=0.8->flask-ngrok) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from Flask>=0.8->flask-ngrok) (8.3.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask>=0.8->flask-ngrok) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->flask-ngrok) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->flask-ngrok) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->flask-ngrok) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->flask-ngrok) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFvSpRqjXjz7",
        "outputId": "ce89c9be-d3e2-4dee-9baf-e4042458c796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Downloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken export NGROK_AUTHTOKEN=369xKmNANRGTKNDzkIQ6fIJHq8R_4h5Xip4mHi3Lcu5TJ1Ry3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcSuuviHYXaJ",
        "outputId": "63af04be-711f-4b81-bb83-512ce3af7222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR:  accepts 1 arg(s), received 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 $(lsof -t -i:5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6WzPrtAY_cJ",
        "outputId": "cd346bfd-1858-4a50-ce03-0d6e8e848c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -q\n",
        "import gradio as gr\n",
        "\n",
        "# Make sure these exist from your training cell:\n",
        "# log_reg_model, tfidf_vectorizer, preprocess_text, compute_fit_score\n",
        "\n",
        "def compute_fit_score(resume_text: str, job_description: str) -> float:\n",
        "    resume_clean = preprocess_text(resume_text)\n",
        "    jd_clean = preprocess_text(job_description)\n",
        "    if not resume_clean or not jd_clean:\n",
        "        return 0.0\n",
        "    vecs = tfidf_vectorizer.transform([jd_clean, resume_clean])\n",
        "    return float(cosine_similarity(vecs[0], vecs[1])[0][0] * 100)\n",
        "\n",
        "def rank_resumes(job_description, resumes_input):\n",
        "    if not job_description.strip() or not resumes_input.strip():\n",
        "        return \"Please enter a job description and at least one resume.\"\n",
        "\n",
        "    resumes = [r.strip() for r in resumes_input.split(\"|||\") if r.strip()]\n",
        "    scored = []\n",
        "\n",
        "    for idx, txt in enumerate(resumes):\n",
        "        score = compute_fit_score(txt, job_description)\n",
        "        pred_label = log_reg_model.predict(\n",
        "            tfidf_vectorizer.transform([preprocess_text(txt)])\n",
        "        )[0]\n",
        "        scored.append((idx + 1, pred_label, round(score, 2), txt))\n",
        "\n",
        "    # sort by score descending\n",
        "    scored.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    lines = []\n",
        "    for idx, label, score, txt in scored:\n",
        "        snippet = txt.replace(\"\\n\", \" \")\n",
        "        if len(snippet) > 100:\n",
        "            snippet = snippet[:100] + \"...\"\n",
        "        lines.append(f\"{idx}. {label} | Score: {score}% | {snippet}\")\n",
        "\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "ui = gr.Interface(\n",
        "    fn=rank_resumes,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=6, label=\"Job Description\",\n",
        "                   placeholder=\"Paste job description here...\"),\n",
        "        gr.Textbox(lines=10, label='Resumes (separate using \\\"|||\\\")',\n",
        "                   placeholder='Resume 1 text ||| Resume 2 text ||| Resume 3 text')\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"AI Resume Screening\"\n",
        ")\n",
        "\n",
        "ui.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "WIHeNOxGZGF3",
        "outputId": "e72a3d44-233c-43b7-af37-020c7ad7fc20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://db7065ecc6a7906c88.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://db7065ecc6a7906c88.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}